<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SEED: Towards More Accurate Semantic Evaluation for Visual Brain Decoding</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <style>
    .seed-tabs {
      display: flex;
      justify-content: center;
      gap: 0.75rem;
      flex-wrap: wrap;
      margin-top: 1rem;
    }

    .seed-tab-button {
      border: 2px solid #dbdbdb;
      background: #ffffff;
      color: #363636;
      border-radius: 999px;
      padding: 0.5rem 1rem;
      font-weight: 700;
      cursor: pointer;
      transition: all 0.2s ease;
    }

    .seed-tab-button.is-active {
      border-color: #3273dc;
      background: #3273dc;
      color: #ffffff;
    }

    .seed-panel {
      display: none;
      margin-top: 1.25rem;
    }

    .seed-panel.is-active {
      display: block;
    }
  </style>
  <link rel="icon" href="./static/images/mind_logo_cut.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SEED: Towards More Accurate Semantic Evaluation for Visual Brain Decoding</h1>
          <p class="subtitle is-4" style="margin-top: 0.5rem; margin-bottom: 0.75rem;"><strong>ICLR 2026</strong></p>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Juhyeon Park<sup>1</sup>,</span>
            <span class="author-block">
              Peter Yongho Kim<sup>1</sup>,</span>
            <span class="author-block">
              Jiook Cha<sup>1</sup>,
            </span>
            <span class="author-block">
              Shinjae Yoo<sup>2</sup>,
            </span>
            <span class="author-block">
              Taesup Moon<sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Seoul National University,</span>
            <span class="author-block"><sup>2</sup>Brookhaven National Laboratory</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Concarne2/SEED"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">A New Evaluation Method for Visual Decoding Models</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <img src="./static/images/seed_main_fig.png" alt="Main Figure" style="max-width: 100%; height: auto;">
          <p class="subtitle is-6 has-text-centered" style="margin-top: 1rem;">
            We introduce new evaluation metrics for visual decoding models, geared towards evaluating the semantics of the images.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present SEED (<strong>Se</strong>mantic <strong>E</strong>valuation for Visual Brain <strong>D</strong>ecoding), a novel metric for evaluating the semantic decoding performance of visual brain decoding models. It integrates three complementary metrics, each capturing a different aspect of semantic similarity between images inspired by neuroscientific findings. Using carefully crowd-sourced human evaluation data, we demonstrate that SEED achieves the highest alignment with human evaluation, outperforming other widely used metrics.
          </p>
          <p>
            Through the evaluation of existing visual brain decoding models with SEED, we further reveal that crucial information is often lost in translation, even in the state-of-the-art models that achieve near-perfect scores on existing metrics. This finding highlights the limitations of current evaluation practices and provides guidance for future improvements in decoding models.
          </p>
          <p>
            Finally, to facilitate further research, we open-source the human evaluation data, encouraging the development of more advanced evaluation methods for brain decoding.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Motivation. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation: Problems with Existing Evaluation</h2>
        <div class="content has-text-justified">
          <p>
            While visual decoding models are evaluated using various metrics, they boil down to either low-level pixel-wise similarity (PixCorr, SSIM) or directly comparing abstract features from vision models (AlexNet, Inception, CLIP, EffNet, etc.). 
            These metrics often fail to capture the semantic similarity between images, which decoding models struggle with even to this day.
            Furthermore, the evaluation processes are abstract, making it difficult to pinpoint and diagnose specific issues of the models.
          </p>
          <p>
            To that end, we propose new evaluation metrics that better capture semantic similarity, and provide more interpretable insights into the performance of visual decoding models.
          </p>
        </div>
      </div>
    </div>
    <!--/ Motivation. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">SEED</h2>
        <p class="subtitle is-5 has-text-centered" style="margin-top: 0.75rem; margin-bottom: 0.75rem;">
          SEED combines three complementary metrics to evaluate semantic decoding quality. <strong>Object F1</strong> and <strong>Caption Similarity</strong> are newly proposed in this work.
        </p>
        <p class="has-text-centered" style="font-size: 2.0rem; font-weight: 700; margin-top: 1.25rem; line-height: 1.2;">
          SEED = (<strong>Object F1</strong> + <strong>Caption Similarity</strong> + EffNet)
        </p>
        </div>
      </div>
    <!--/ Abstract. -->
    <!-- SEED Components. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- <h2 class="title is-3 has-text-centered">SEED Components</h2> -->
        <div class="seed-tab-group">
          <div class="seed-tabs" role="tablist" aria-label="SEED metric components">
            <button class="seed-tab-button is-active" type="button" role="tab" aria-selected="true" data-seed-target="seed-panel-object">
              Object F1
            </button>
            <button class="seed-tab-button" type="button" role="tab" aria-selected="false" data-seed-target="seed-panel-caption">
              Caption Similarity
            </button>
            <button class="seed-tab-button" type="button" role="tab" aria-selected="false" data-seed-target="seed-panel-effnet">
              EffNet
            </button>
          </div>

          <div id="seed-panel-object" class="seed-panel is-active">
            <div class="content">
              <p>
                Using off-the-shelf object detectors, <strong>Object F1</strong> compares the similarity of two images based on the presence of key objects.
              </p>
              <img src="./static/images/object_f1.png" alt="Object F1" style="max-width: 100%; height: auto;">
            </div>
          </div>

          <div id="seed-panel-caption" class="seed-panel">
            <div class="content">
              <p>
                <strong>Caption Similarity</strong> uses captioning models to generate text descriptions, and compares the similarity of the captions.
              </p>
              <img src="./static/images/cap_sim.png" alt="Caption Similarity" style="max-width: 100%; height: auto;">
            </div>
          </div>

          <div id="seed-panel-effnet" class="seed-panel">
            <div class="content">
              <p>
                <strong>EffNet</strong>, a widely adopted metric, compares the similarity of two images in the feature space of an EfficientNet model.
              </p>
              <img src="./static/images/effnet.png" alt="EffNet" style="max-width: 100%; height: auto;">
            </div>
          </div>
        </div>
      </div>
    </div>
    <!--/ SEED Components. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Human Evaluation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Human Evaluation</h2>
        <div class="content">
          <p>
            We collected crowd-sourced human judgments on the similarity of two images, using reconstructions from MindEye2, to measure how well evaluation metrics align with human judgments. Here is how the human judgments were collected:
          </p>
          <img src="./static/images/mturk_screenshot.png" alt="MTurk human evaluation interface screenshot" style="max-width: 85%; height: auto; display: block; margin: 0 auto;">
        </div>
      </div>
    </div>
    <!--/ Human Evaluation. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Alignment Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Alignment Results</h2>
        <div class="content">
          <p>
            The table reports alignment between different metrics and the collected human judgments. <strong>SEED</strong> achieves the highest alignment across all three measures, outperforming its three component metrics.
          </p>
        </div>
        <div class="table-container" style="width: 80%; margin: 0 auto;">
          <table class="table is-fullwidth is-striped">
            <thead>
              <tr>
                <th>Metric</th>
                <th class="has-text-centered">Pairwise Accuracy (&uarr;)</th>
                <th class="has-text-centered">Kendall's tau (&uarr;)</th>
                <th class="has-text-centered">Pearson correlation (&uarr;)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>PixCorr</td>
                <td class="has-text-centered">53.8%</td>
                <td class="has-text-centered">.075</td>
                <td class="has-text-centered">.117</td>
              </tr>
              <tr>
                <td>SSIM</td>
                <td class="has-text-centered">54.5%</td>
                <td class="has-text-centered">.090</td>
                <td class="has-text-centered">.112</td>
              </tr>
              <tr>
                <td>AlexNet(2)</td>
                <td class="has-text-centered">55.0%</td>
                <td class="has-text-centered">.185</td>
                <td class="has-text-centered">.187</td>
              </tr>
              <tr>
                <td>AlexNet(5)</td>
                <td class="has-text-centered">49.5%</td>
                <td class="has-text-centered">.236</td>
                <td class="has-text-centered">.258</td>
              </tr>
              <tr style="border-top: 2px solid #b5b5b5;">
                <td>Inception</td>
                <td class="has-text-centered">63.8%</td>
                <td class="has-text-centered">.330</td>
                <td class="has-text-centered">.475</td>
              </tr>
              <tr>
                <td>CLIP</td>
                <td class="has-text-centered">66.4%</td>
                <td class="has-text-centered">.368</td>
                <td class="has-text-centered">.436</td>
              </tr>
              <tr>
                <td><span style="text-decoration: overline;">EffNet</span></td>
                <td class="has-text-centered">78.0%</td>
                <td class="has-text-centered">.559</td>
                <td class="has-text-centered">.748</td>
              </tr>
              <tr>
                <td><span style="text-decoration: overline;">SwAV</span></td>
                <td class="has-text-centered">69.7%</td>
                <td class="has-text-centered">.394</td>
                <td class="has-text-centered">.576</td>
              </tr>
              <tr style="border-top: 2px solid #b5b5b5;">
                <td>Object F1 (Ours)</td>
                <td class="has-text-centered">75.8%</td>
                <td class="has-text-centered">.516</td>
                <td class="has-text-centered">.708</td>
              </tr>
              <tr>
                <td>Caption Similarity (Ours)</td>
                <td class="has-text-centered">73.8%</td>
                <td class="has-text-centered">.477</td>
                <td class="has-text-centered">.666</td>
              </tr>
              <tr>
                <td><strong>SEED (Ours)</strong></td>
                <td class="has-text-centered"><strong>81.0%</strong></td>
                <td class="has-text-centered"><strong>.621</strong></td>
                <td class="has-text-centered"><strong>.813</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
    </div>
    <!--/ Alignment Results. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Additional Analysis. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Common Failure Modes of Decoding Models</h2>
        <div class="seed-tab-group">
          <div class="seed-tabs" role="tablist" aria-label="Additional analysis options">
            <button class="seed-tab-button is-active" type="button" role="tab" aria-selected="true" data-seed-target="analysis-panel-backbone">
              Semantic Near-miss
            </button>
            <button class="seed-tab-button" type="button" role="tab" aria-selected="false" data-seed-target="analysis-panel-option2">
              Semantic Detail Mismatch
            </button>
          </div>

          <div id="analysis-panel-backbone" class="seed-panel is-active">
            <div class="content">
              <p>
                Decoding models often capture the broad supercategory correctly, but misrepresent the specific object category. 
                <br>
                We call this the <strong>semantic near-miss</strong> phenomenon, which was quantifiable thanks to the object detection pipeline.
                <br>
                For example, a model might reconstruct a different animal for a photo of a dog.
              </p>
              <img src="./static/images/semantic_near_miss.png" alt="Semantic near-miss examples" style="max-width: 80%; height: auto; display: block; margin: 0 auto;">
            </div>
          </div>

          <div id="analysis-panel-option2" class="seed-panel">
            <div class="content">
              <p>
                Decoding models sometimes only capture the but not the details such as background, pose, or color. 
                This detail mismatch was quantifiable by finding cases where <strong>Object F1</strong> and <strong>SEED</strong> scores have a large discrepancy.
              </p>
              <img src="./static/images/semantic_detail_mismatch.png" alt="Semantic detail mismatch examples" style="max-width: 80%; height: auto; display: block; margin: 0 auto;">
            </div>
          </div>
        </div>
      </div>
    </div>
    <!--/ Additional Analysis. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website was built using code from the
            <a href="https://github.com/nerfies/nerfies.github.io">Nerfies website</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    var tabGroups = document.querySelectorAll('.seed-tab-group');

    if (!tabGroups.length) {
      return;
    }

    tabGroups.forEach(function(group) {
      var tabButtons = group.querySelectorAll('.seed-tab-button');
      var panels = group.querySelectorAll('.seed-panel');

      tabButtons.forEach(function(button) {
        button.addEventListener('click', function() {
          var targetId = button.getAttribute('data-seed-target');
          var targetPanel = group.querySelector('#' + targetId);

          tabButtons.forEach(function(btn) {
            btn.classList.remove('is-active');
            btn.setAttribute('aria-selected', 'false');
          });

          panels.forEach(function(panel) {
            panel.classList.remove('is-active');
          });

          button.classList.add('is-active');
          button.setAttribute('aria-selected', 'true');

          if (targetPanel) {
            targetPanel.classList.add('is-active');
          }
        });
      });
    });
  });
</script>

</body>
</html>
